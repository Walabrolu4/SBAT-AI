{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LO4ULZvOsIuX"
      },
      "source": [
        "#\n",
        "Colab Setup:\n",
        "1. Create a new Colab notebook.\n",
        "2. Install necessary Python packages:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "x_GY7MAYrtrc"
      },
      "outputs": [],
      "source": [
        "!pip install flask flask-cors pyngrok google-generativeai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylnwS11stZwN"
      },
      "source": [
        "## Import libraries and configure your LLM API key (using Colab's Secrets manager is recommended for keys):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sn7M4yY3tS2q"
      },
      "outputs": [],
      "source": [
        "import google.generativeai as genai\n",
        "from google.colab import userdata # For secrets\n",
        "import json\n",
        "# Assuming you've stored your API key as 'GEMINI_API_KEY' in Colab secrets\n",
        "try:\n",
        "    api_key = userdata.get('GEMINI_API_KEY')\n",
        "    genai.configure(api_key=api_key)\n",
        "    print(\"Gemini API Key configured.\")\n",
        "except userdata.SecretNotFoundError:\n",
        "    print(\"ERROR: Secret 'GEMINI_API_KEY' not found. Please add it in Colab's Secrets manager (key icon on the left).\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during API key configuration: {e}\")\n",
        "\n",
        "# Placeholder for the model\n",
        "llm_model = None\n",
        "if 'api_key' in locals() and api_key:\n",
        "     try:\n",
        "         llm_model = genai.GenerativeModel('gemini-1.5-flash') # Or another suitable model\n",
        "         print(\"Gemini model initialized.\")\n",
        "     except Exception as e:\n",
        "         print(f\"Error initializing Gemini model: {e}\")\n",
        "else:\n",
        "    print(\"Skipping model initialization due to missing API key.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zD6q_4K9SzdA"
      },
      "source": [
        "#Setup Imports and Run Flask Server\n",
        "\n",
        "## setup imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xte0nX6hSzLj"
      },
      "outputs": [],
      "source": [
        "from flask import Flask, request, jsonify, send_from_directory\n",
        "from flask_cors import CORS\n",
        "import threading\n",
        "import os\n",
        "from pyngrok import ngrok # Import ngrok\n",
        "import re # For parsing LLM responses\n",
        "!mkdir -p game_files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "maG5NehLTYSm"
      },
      "source": [
        "## Flask App Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1HIh92exTbfz",
        "outputId": "25e9d008-3688-4a96-c458-e2f6d07a90ed"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<flask_cors.extension.CORS at 0x79cdac587350>"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "app = Flask(__name__, static_folder = 'game_files')\n",
        "CORS(app)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xb2avmJNVfmA"
      },
      "source": [
        "## Game Logic and placeholder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11snrg-zVqT5"
      },
      "source": [
        "## LLM Interaction Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fBNGxG3cVtI0",
        "outputId": "b8070d97-135c-49e1-af2c-1ec23ce4fd3a"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\ndef get_llm_decision(game_state, available_actions, user_instruction):\\n    if not llm_model:\\n         return {\"error\": \"LLM model not initialized.\"}\\n\\n    # --- Crucial: Prompt Engineering ---\\n    # Provide context, rules, state, available actions, and the user\\'s goal.\\n    # Ask for the output in a specific, parseable format (like JSON).\\n    prompt = f\"\"\"You are playing a game. Respond with ONLY the chosen action in the format specified.\\n\\n    Game: Tic-Tac-Toe\\n    Your Mark: O (You are playing as O)\\n    Player Mark: X\\n\\n    Current Board State:\\n    {json.dumps(game_state[\\'board\\'], indent=2)}\\n    (Empty strings \"\" represent empty cells)\\n\\n    Available Actions (Choose one by coordinates row, col):\\n    {json.dumps(available_actions, indent=2)}\\n\\n    User Instruction: \"{user_instruction}\"\\n\\n    Game Status: {game_state[\\'status\\']}\\n    Current Turn: {game_state[\\'turn\\']}\\n\\n    Based on the user instruction and the current state, choose the best action from the available actions for player \\'O\\'.\\n    Respond ONLY with the chosen action as a JSON object like this: {{\"action\": \"place O at (row, col)\"}} or like this if you need coordinates: {{\"row\": r, \"col\": c}}\\n    Example response for placing O in the top-left: {{\"row\": 0, \"col\": 0}}\\n    \"\"\"\\n\\n    print(\"--- Sending Prompt to LLM ---\")\\n    # print(prompt) # Uncomment to debug the prompt\\n\\n    try:\\n        response = llm_model.generate_content(prompt)\\n        print(\"--- Received Response from LLM ---\")\\n        print(response.text)\\n\\n        # --- Parse the LLM Response ---\\n        # This is critical and might need robust error handling\\n        try:\\n            # Try parsing directly if LLM gives perfect JSON (unlikely)\\n            # action_data = json.loads(response.text.strip())\\n\\n            # More robust: Search for JSON-like structure or extract coords\\n            # Example naive extraction (adapt based on LLM output format):\\n            import re\\n            match = re.search(r\\'{\\\\s*\"row\":\\\\s*(\\\\d+),\\\\s*\"col\":\\\\s*(\\\\d+)\\\\s*}\\', response.text)\\n            if match:\\n                row, col = int(match.group(1)), int(match.group(2))\\n                # Validate if this action is actually in available_actions\\n                if {\"row\": row, \"col\": col} in available_actions:\\n                    print(f\"LLM chose valid action: row={row}, col={col}\")\\n                    return {\"row\": row, \"col\": col}\\n                else:\\n                     print(f\"LLM chose an INVALID action ({row},{col}), not in available actions.\")\\n                     return {\"error\": f\"LLM chose an invalid action: ({row},{col})\"}\\n\\n            # Fallback or different parsing if needed\\n            return {\"error\": \"Could not parse valid action from LLM response.\", \"raw_response\": response.text}\\n\\n        except json.JSONDecodeError:\\n             print(f\"LLM response was not valid JSON: {response.text}\")\\n             return {\"error\": \"LLM response was not valid JSON.\", \"raw_response\": response.text}\\n        except Exception as e:\\n             print(f\"Error parsing LLM response: {e}\")\\n             return {\"error\": f\"Error parsing LLM response: {e}\", \"raw_response\": response.text}\\n\\n    except Exception as e:\\n        print(f\"Error calling LLM API: {e}\")\\n        return {\"error\": f\"Error calling LLM API: {e}\"}\\n        '"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''\n",
        "def get_llm_decision(game_state, available_actions, user_instruction):\n",
        "    if not llm_model:\n",
        "         return {\"error\": \"LLM model not initialized.\"}\n",
        "\n",
        "    # --- Crucial: Prompt Engineering ---\n",
        "    # Provide context, rules, state, available actions, and the user's goal.\n",
        "    # Ask for the output in a specific, parseable format (like JSON).\n",
        "    prompt = f\"\"\"You are playing a game. Respond with ONLY the chosen action in the format specified.\n",
        "\n",
        "    Game: Tic-Tac-Toe\n",
        "    Your Mark: O (You are playing as O)\n",
        "    Player Mark: X\n",
        "\n",
        "    Current Board State:\n",
        "    {json.dumps(game_state['board'], indent=2)}\n",
        "    (Empty strings \"\" represent empty cells)\n",
        "\n",
        "    Available Actions (Choose one by coordinates row, col):\n",
        "    {json.dumps(available_actions, indent=2)}\n",
        "\n",
        "    User Instruction: \"{user_instruction}\"\n",
        "\n",
        "    Game Status: {game_state['status']}\n",
        "    Current Turn: {game_state['turn']}\n",
        "\n",
        "    Based on the user instruction and the current state, choose the best action from the available actions for player 'O'.\n",
        "    Respond ONLY with the chosen action as a JSON object like this: {{\"action\": \"place O at (row, col)\"}} or like this if you need coordinates: {{\"row\": r, \"col\": c}}\n",
        "    Example response for placing O in the top-left: {{\"row\": 0, \"col\": 0}}\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"--- Sending Prompt to LLM ---\")\n",
        "    # print(prompt) # Uncomment to debug the prompt\n",
        "\n",
        "    try:\n",
        "        response = llm_model.generate_content(prompt)\n",
        "        print(\"--- Received Response from LLM ---\")\n",
        "        print(response.text)\n",
        "\n",
        "        # --- Parse the LLM Response ---\n",
        "        # This is critical and might need robust error handling\n",
        "        try:\n",
        "            # Try parsing directly if LLM gives perfect JSON (unlikely)\n",
        "            # action_data = json.loads(response.text.strip())\n",
        "\n",
        "            # More robust: Search for JSON-like structure or extract coords\n",
        "            # Example naive extraction (adapt based on LLM output format):\n",
        "            import re\n",
        "            match = re.search(r'{\\s*\"row\":\\s*(\\d+),\\s*\"col\":\\s*(\\d+)\\s*}', response.text)\n",
        "            if match:\n",
        "                row, col = int(match.group(1)), int(match.group(2))\n",
        "                # Validate if this action is actually in available_actions\n",
        "                if {\"row\": row, \"col\": col} in available_actions:\n",
        "                    print(f\"LLM chose valid action: row={row}, col={col}\")\n",
        "                    return {\"row\": row, \"col\": col}\n",
        "                else:\n",
        "                     print(f\"LLM chose an INVALID action ({row},{col}), not in available actions.\")\n",
        "                     return {\"error\": f\"LLM chose an invalid action: ({row},{col})\"}\n",
        "\n",
        "            # Fallback or different parsing if needed\n",
        "            return {\"error\": \"Could not parse valid action from LLM response.\", \"raw_response\": response.text}\n",
        "\n",
        "        except json.JSONDecodeError:\n",
        "             print(f\"LLM response was not valid JSON: {response.text}\")\n",
        "             return {\"error\": \"LLM response was not valid JSON.\", \"raw_response\": response.text}\n",
        "        except Exception as e:\n",
        "             print(f\"Error parsing LLM response: {e}\")\n",
        "             return {\"error\": f\"Error parsing LLM response: {e}\", \"raw_response\": response.text}\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error calling LLM API: {e}\")\n",
        "        return {\"error\": f\"Error calling LLM API: {e}\"}\n",
        "        '''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S95L3C_UqqBl"
      },
      "outputs": [],
      "source": [
        "# server.py or llm_controller.py\n",
        "\n",
        "from flask import Flask, request, jsonify\n",
        "import json\n",
        "import re\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "# Mockup LLM object\n",
        "llm_model = ...  # Your LLM API object\n",
        "\n",
        "@app.route(\"/llm/move\", methods=[\"POST\"])\n",
        "def llm_move():\n",
        "    data = request.json\n",
        "    if not llm_model:\n",
        "        return jsonify({\"error\": \"LLM model not initialized.\"})\n",
        "\n",
        "    units = data.get(\"units\", [])\n",
        "    instruction = data.get(\"instruction\", \"\")\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "You are playing a strategy game. Your goal is to move units based on the user's instruction.\n",
        "\n",
        "Units:\n",
        "{json.dumps(units, indent=2)}\n",
        "\n",
        "Each unit has: unitId, x, y, and fuel.\n",
        "\n",
        "User Instruction:\n",
        "\\\"\\\"\\\"{instruction}\\\"\\\"\\\"\n",
        "\n",
        "Respond with a list of move commands. Each command should look like:\n",
        "{{\"unitId\": <number>, \"x\": <number>, \"y\": <number>, \"queue\": <true/false>}}\n",
        "\n",
        "Only return the list of JSON objects. Example:\n",
        "[\n",
        "  {{\"unitId\": 1, \"x\": 3, \"y\": 4, \"queue\": false}},\n",
        "  {{\"unitId\": 2, \"x\": 5, \"y\": 6, \"queue\": true}}\n",
        "]\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"--- LLM Prompt ---\")\n",
        "    print(prompt)\n",
        "\n",
        "    try:\n",
        "        response = llm_model.generate_content(prompt)\n",
        "        raw = response.text.strip()\n",
        "\n",
        "        print(\"--- LLM Response ---\")\n",
        "        print(raw)\n",
        "\n",
        "        try:\n",
        "            move_plan = json.loads(raw)\n",
        "            if isinstance(move_plan, list):\n",
        "                return jsonify({\"plan\": move_plan})\n",
        "            else:\n",
        "                return jsonify({\"error\": \"LLM did not return a list\", \"raw\": raw})\n",
        "        except Exception as e:\n",
        "            return jsonify({\"error\": \"Could not parse LLM output\", \"details\": str(e), \"raw\": raw})\n",
        "\n",
        "    except Exception as e:\n",
        "        return jsonify({\"error\": f\"Error calling LLM API: {e}\"})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1xZzK4LWGLu"
      },
      "source": [
        "## API Endpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CJepvQUtWJnc"
      },
      "outputs": [],
      "source": [
        "@app.route('/api/llm_move', methods=['POST'])\n",
        "def handle_llm_move():\n",
        "    data = request.json\n",
        "    game_state = data.get('gameState')\n",
        "    available_actions = data.get('availableActions') # JS needs to calculate these!\n",
        "    user_instruction = data.get('instruction')\n",
        "\n",
        "    if not game_state or not available_actions or user_instruction is None:\n",
        "        return jsonify({\"error\": \"Missing gameState, availableActions, or instruction\"}), 400\n",
        "\n",
        "    # Basic validation (more needed in a real app)\n",
        "    if game_state.get('currentPlayer') != 'O': # Assuming LLM plays 'O'\n",
        "         return jsonify({\"error\": \"Not LLM's turn\"}), 400\n",
        "    if game_state.get('status') != 'playing':\n",
        "         return jsonify({\"error\": \"Game is not active\"}), 400\n",
        "\n",
        "    decision = get_llm_decision(game_state, available_actions, user_instruction)\n",
        "    return jsonify(decision)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-npMBLdkWNcf"
      },
      "source": [
        "## Static File Serving"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rXXZxo8oWQlB"
      },
      "outputs": [],
      "source": [
        "# --- Static File Serving ---\n",
        "# Create a directory in Colab's file system to hold your game files\n",
        "if not os.path.exists('game_files'):\n",
        "    os.makedirs('game_files')\n",
        "\n",
        "# You need to UPLOAD your index.html, game.js, phaser.min.js, etc.\n",
        "# into this 'game_files' directory using Colab's file browser (folder icon on left).\n",
        "\n",
        "@app.route('/')\n",
        "def index():\n",
        "    # Serves index.html from the 'game_files' directory\n",
        "    return send_from_directory('game_files', 'index.html')\n",
        "\n",
        "@app.route('/<path:filename>')\n",
        "def serve_static(filename):\n",
        "    # Serves other files (game.js, phaser.min.js, assets)\n",
        "    return send_from_directory('game_files', filename)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5QH-gnGWVgI"
      },
      "source": [
        "## Function to Run Flask App"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0c6TiCTYWX68"
      },
      "outputs": [],
      "source": [
        "def run_flask():\n",
        "  # Needs to run on port 8080 for some Colab environments, or choose another\n",
        "  # Ngrok will tunnel to this port\n",
        "  print(\"Starting Flask server...\")\n",
        "  app.run(host='0.0.0.0', port=8880) # Run on all interfaces, port 8080"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9FYy9cBWhBH"
      },
      "source": [
        "## Start ngrok Tunnel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ApvgNIW1WjZ1"
      },
      "outputs": [],
      "source": [
        "def start_ngrok():\n",
        "    try:\n",
        "        # Terminate existing tunnels if any\n",
        "        ngrok.kill()\n",
        "        # Get ngrok auth token from Colab secrets if you have one (recommended for stable URLs)\n",
        "        try:\n",
        "             ngrok_auth_token = userdata.get('NGROK_AUTH_TOKEN')\n",
        "             ngrok.set_auth_token(ngrok_auth_token)\n",
        "             print(\"Ngrok auth token set.\")\n",
        "        except userdata.SecretNotFoundError:\n",
        "             print(\"INFO: NGROK_AUTH_TOKEN secret not found. Using ngrok without auth token (temporary URL).\")\n",
        "        except Exception as e:\n",
        "             print(f\"Error setting ngrok auth token: {e}\")\n",
        "\n",
        "        # Start an HTTP tunnel on the same port Flask is running on\n",
        "        public_url = ngrok.connect(8880, \"http\")\n",
        "        print(f\" * ngrok tunnel available at: {public_url}\")\n",
        "        return public_url\n",
        "    except Exception as e:\n",
        "        print(f\"Error starting ngrok: {e}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3wArGg1WsOb"
      },
      "source": [
        "## Main Execution Logic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "xcCGmv2RWuXY",
        "outputId": "d3532ae0-64b9-4c89-ed6b-e69dfaf7e3b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error setting ngrok auth token: Requesting secret NGROK_AUTH_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR:pyngrok.process.ngrok:t=2025-06-10T03:14:56+0000 lvl=eror msg=\"failed to reconnect session\" obj=tunnels.session err=\"authentication failed: Usage of ngrok requires a verified account and authtoken.\\n\\nSign up for an account: https://dashboard.ngrok.com/signup\\nInstall your authtoken: https://dashboard.ngrok.com/get-started/your-authtoken\\r\\n\\r\\nERR_NGROK_4018\\r\\n\"\n",
            "ERROR:pyngrok.process.ngrok:t=2025-06-10T03:14:56+0000 lvl=eror msg=\"session closing\" obj=tunnels.session err=\"authentication failed: Usage of ngrok requires a verified account and authtoken.\\n\\nSign up for an account: https://dashboard.ngrok.com/signup\\nInstall your authtoken: https://dashboard.ngrok.com/get-started/your-authtoken\\r\\n\\r\\nERR_NGROK_4018\\r\\n\"\n",
            "ERROR:pyngrok.process.ngrok:t=2025-06-10T03:14:56+0000 lvl=eror msg=\"terminating with error\" obj=app err=\"authentication failed: Usage of ngrok requires a verified account and authtoken.\\n\\nSign up for an account: https://dashboard.ngrok.com/signup\\nInstall your authtoken: https://dashboard.ngrok.com/get-started/your-authtoken\\r\\n\\r\\nERR_NGROK_4018\\r\\n\"\n",
            "CRITICAL:pyngrok.process.ngrok:t=2025-06-10T03:14:56+0000 lvl=crit msg=\"command failed\" err=\"authentication failed: Usage of ngrok requires a verified account and authtoken.\\n\\nSign up for an account: https://dashboard.ngrok.com/signup\\nInstall your authtoken: https://dashboard.ngrok.com/get-started/your-authtoken\\r\\n\\r\\nERR_NGROK_4018\\r\\n\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error starting ngrok: The ngrok process errored on start: authentication failed: Usage of ngrok requires a verified account and authtoken.\\n\\nSign up for an account: https://dashboard.ngrok.com/signup\\nInstall your authtoken: https://dashboard.ngrok.com/get-started/your-authtoken\\r\\n\\r\\nERR_NGROK_4018\\r\\n.\n",
            "\n",
            "--- Server Setup Complete ---\n",
            "Failed to start ngrok tunnel. Server might be running but not accessible publicly.\n",
            "Make sure you have uploaded your index.html, game.js, and phaser.min.js into the 'game_files' directory in Colab.\n",
            "\n",
            "--- Starting Flask Server (Foreground - Blocking) ---\n",
            "Flask is now running and listening on port 8880.\n",
            "Logs should appear directly below as requests come in.\n",
            "!!! The Colab cell will appear 'busy'. To stop the server, you MUST interrupt the kernel !!!\n",
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on all addresses (0.0.0.0)\n",
            " * Running on http://127.0.0.1:8880\n",
            " * Running on http://172.28.0.12:8880\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "\n",
        "public_url = start_ngrok()\n",
        "\n",
        "print(\"\\n--- Server Setup Complete ---\")\n",
        "if public_url:\n",
        "    print(f\"Access your game POC at: {public_url}\")\n",
        "    print(f\"LLM endpoint will be at: {public_url}/api/llm_move\") # Print full URL\n",
        "else:\n",
        "    print(\"Failed to start ngrok tunnel. Server might be running but not accessible publicly.\")\n",
        "    # Maybe exit or raise error if ngrok fails?\n",
        "\n",
        "print(\"Make sure you have uploaded your index.html, game.js, and phaser.min.js into the 'game_files' directory in Colab.\")\n",
        "\n",
        "\n",
        "# --- Run Flask in Foreground (BLOCKING) ---\n",
        "# This will block the cell from 'finishing' until you manually interrupt it (e.g., Runtime -> Interrupt execution).\n",
        "print(\"\\n--- Starting Flask Server (Foreground - Blocking) ---\")\n",
        "print(f\"Flask is now running and listening on port 8880.\")\n",
        "print(\"Logs should appear directly below as requests come in.\")\n",
        "print(\"!!! The Colab cell will appear 'busy'. To stop the server, you MUST interrupt the kernel !!!\")\n",
        "try:\n",
        "    # Run Flask on 0.0.0.0 to accept connections from ngrok\n",
        "    app.run(host='0.0.0.0', port=8880, debug=False) # Turn debug=True for more Flask logs if needed, but can be verbose\n",
        "except KeyboardInterrupt:\n",
        "    print(\"\\n--- Flask server stopped (KeyboardInterrupt) ---\")\n",
        "\n",
        "# The code below this line will NOT execute until the server is stopped.\n",
        "print(\"--- Flask server has been shut down ---\")\n",
        "\n",
        "\n",
        "'''\n",
        "# Start Flask in a separate thread so it doesn't block Colab execution\n",
        "flask_thread = threading.Thread(target=run_flask, daemon=True)\n",
        "flask_thread.start()\n",
        "\n",
        "# Start ngrok and print the public URL\n",
        "public_url = start_ngrok()\n",
        "\n",
        "print(\"\\n--- Server Setup Complete ---\")\n",
        "if public_url:\n",
        "    print(f\"Access your game POC at: {public_url}\")\n",
        "else:\n",
        "    print(\"Failed to start ngrok tunnel. Server might be running but not accessible publicly.\")\n",
        "print(\"Flask server is running in the background.\")\n",
        "print(\"LLM endpoint is available at /api/llm_move (relative to the ngrok URL)\")\n",
        "print(\"Make sure you have uploaded your index.html, game.js, and phaser.min.js into the 'game_files' directory in Colab.\")\n",
        "# Keep the Colab cell running..\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_W_eA3wyWyRw"
      },
      "source": [
        "## EXTRA SHIT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OyX2sTCl0ri3"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "from flask import Flask, request, jsonify, send_from_directory\n",
        "from flask_cors import CORS\n",
        "import threading\n",
        "import os\n",
        "from pyngrok import ngrok # Import ngrok\n",
        "\n",
        "# --- Flask App Setup ---\n",
        "app = Flask(__name__, static_folder='game_files') # Point static folder\n",
        "CORS(app) # Enable Cross-Origin Resource Sharing\n",
        "\n",
        "# --- Game Logic Placeholder (will be refined) ---\n",
        "# Store game state server-side IF needed, or manage fully client-side and send state with each request\n",
        "# For this POC, let's assume the client (Phaser) manages state and sends it.\n",
        "\n",
        "# --- LLM Interaction Function ---\n",
        "def get_llm_decision(game_state, available_actions, user_instruction):\n",
        "    if not llm_model:\n",
        "         return {\"error\": \"LLM model not initialized.\"}\n",
        "\n",
        "    # --- Crucial: Prompt Engineering ---\n",
        "    # Provide context, rules, state, available actions, and the user's goal.\n",
        "    # Ask for the output in a specific, parseable format (like JSON).\n",
        "    prompt = f\"\"\"You are playing a game. Respond with ONLY the chosen action in the format specified.\n",
        "\n",
        "    Game: Tic-Tac-Toe\n",
        "    Your Mark: O (You are playing as O)\n",
        "    Player Mark: X\n",
        "\n",
        "    Current Board State:\n",
        "    {json.dumps(game_state['board'], indent=2)}\n",
        "    (Empty strings \"\" represent empty cells)\n",
        "\n",
        "    Available Actions (Choose one by coordinates row, col):\n",
        "    {json.dumps(available_actions, indent=2)}\n",
        "\n",
        "    User Instruction: \"{user_instruction}\"\n",
        "\n",
        "    Game Status: {game_state['status']}\n",
        "    Current Turn: {game_state['turn']}\n",
        "\n",
        "    Based on the user instruction and the current state, choose the best action from the available actions for player 'O'.\n",
        "    Respond ONLY with the chosen action as a JSON object like this: {{\"action\": \"place O at (row, col)\"}} or like this if you need coordinates: {{\"row\": r, \"col\": c}}\n",
        "    Example response for placing O in the top-left: {{\"row\": 0, \"col\": 0}}\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"--- Sending Prompt to LLM ---\")\n",
        "    # print(prompt) # Uncomment to debug the prompt\n",
        "\n",
        "    try:\n",
        "        response = llm_model.generate_content(prompt)\n",
        "        print(\"--- Received Response from LLM ---\")\n",
        "        print(response.text)\n",
        "\n",
        "        # --- Parse the LLM Response ---\n",
        "        # This is critical and might need robust error handling\n",
        "        try:\n",
        "            # Try parsing directly if LLM gives perfect JSON (unlikely)\n",
        "            # action_data = json.loads(response.text.strip())\n",
        "\n",
        "            # More robust: Search for JSON-like structure or extract coords\n",
        "            # Example naive extraction (adapt based on LLM output format):\n",
        "            import re\n",
        "            match = re.search(r'{\\s*\"row\":\\s*(\\d+),\\s*\"col\":\\s*(\\d+)\\s*}', response.text)\n",
        "            if match:\n",
        "                row, col = int(match.group(1)), int(match.group(2))\n",
        "                # Validate if this action is actually in available_actions\n",
        "                if {\"row\": row, \"col\": col} in available_actions:\n",
        "                    print(f\"LLM chose valid action: row={row}, col={col}\")\n",
        "                    return {\"row\": row, \"col\": col}\n",
        "                else:\n",
        "                     print(f\"LLM chose an INVALID action ({row},{col}), not in available actions.\")\n",
        "                     return {\"error\": f\"LLM chose an invalid action: ({row},{col})\"}\n",
        "\n",
        "            # Fallback or different parsing if needed\n",
        "            return {\"error\": \"Could not parse valid action from LLM response.\", \"raw_response\": response.text}\n",
        "\n",
        "        except json.JSONDecodeError:\n",
        "             print(f\"LLM response was not valid JSON: {response.text}\")\n",
        "             return {\"error\": \"LLM response was not valid JSON.\", \"raw_response\": response.text}\n",
        "        except Exception as e:\n",
        "             print(f\"Error parsing LLM response: {e}\")\n",
        "             return {\"error\": f\"Error parsing LLM response: {e}\", \"raw_response\": response.text}\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error calling LLM API: {e}\")\n",
        "        return {\"error\": f\"Error calling LLM API: {e}\"}\n",
        "\n",
        "# --- API Endpoint ---\n",
        "@app.route('/api/llm_move', methods=['POST'])\n",
        "def handle_llm_move():\n",
        "    data = request.json\n",
        "    game_state = data.get('gameState')\n",
        "    available_actions = data.get('availableActions') # JS needs to calculate these!\n",
        "    user_instruction = data.get('instruction')\n",
        "\n",
        "    if not game_state or not available_actions or user_instruction is None:\n",
        "        return jsonify({\"error\": \"Missing gameState, availableActions, or instruction\"}), 400\n",
        "\n",
        "    # Basic validation (more needed in a real app)\n",
        "    if game_state.get('turn') != 'O': # Assuming LLM plays 'O'\n",
        "         return jsonify({\"error\": \"Not LLM's turn\"}), 400\n",
        "    if game_state.get('status') != 'playing':\n",
        "         return jsonify({\"error\": \"Game is not active\"}), 400\n",
        "\n",
        "    decision = get_llm_decision(game_state, available_actions, user_instruction)\n",
        "    return jsonify(decision)\n",
        "\n",
        "# --- Static File Serving ---\n",
        "# Create a directory in Colab's file system to hold your game files\n",
        "if not os.path.exists('game_files'):\n",
        "    os.makedirs('game_files')\n",
        "\n",
        "# You need to UPLOAD your index.html, game.js, phaser.min.js, etc.\n",
        "# into this 'game_files' directory using Colab's file browser (folder icon on left).\n",
        "\n",
        "@app.route('/')\n",
        "def index():\n",
        "    # Serves index.html from the 'game_files' directory\n",
        "    return send_from_directory('game_files', 'index.html')\n",
        "\n",
        "@app.route('/<path:filename>')\n",
        "def serve_static(filename):\n",
        "    # Serves other files (game.js, phaser.min.js, assets)\n",
        "    return send_from_directory('game_files', filename)\n",
        "\n",
        "# --- Function to Run Flask App ---\n",
        "def run_flask():\n",
        "  # Needs to run on port 8080 for some Colab environments, or choose another\n",
        "  # Ngrok will tunnel to this port\n",
        "  print(\"Starting Flask server...\")\n",
        "  app.run(host='0.0.0.0', port=8880) # Run on all interfaces, port 8080\n",
        "\n",
        "# --- Start ngrok tunnel ---\n",
        "def start_ngrok():\n",
        "    try:\n",
        "        # Terminate existing tunnels if any\n",
        "        ngrok.kill()\n",
        "        # Get ngrok auth token from Colab secrets if you have one (recommended for stable URLs)\n",
        "        try:\n",
        "             ngrok_auth_token = userdata.get('NGROK_AUTH_TOKEN')\n",
        "             ngrok.set_auth_token(ngrok_auth_token)\n",
        "             print(\"Ngrok auth token set.\")\n",
        "        except userdata.SecretNotFoundError:\n",
        "             print(\"INFO: NGROK_AUTH_TOKEN secret not found. Using ngrok without auth token (temporary URL).\")\n",
        "        except Exception as e:\n",
        "             print(f\"Error setting ngrok auth token: {e}\")\n",
        "\n",
        "        # Start an HTTP tunnel on the same port Flask is running on\n",
        "        public_url = ngrok.connect(8880, \"http\")\n",
        "        print(f\" * ngrok tunnel available at: {public_url}\")\n",
        "        return public_url\n",
        "    except Exception as e:\n",
        "        print(f\"Error starting ngrok: {e}\")\n",
        "        return None\n",
        "\n",
        "# --- Main Execution Logic ---\n",
        "# Start Flask in a separate thread so it doesn't block Colab execution\n",
        "flask_thread = threading.Thread(target=run_flask, daemon=True)\n",
        "flask_thread.start()\n",
        "\n",
        "# Start ngrok and print the public URL\n",
        "public_url = start_ngrok()\n",
        "\n",
        "print(\"\\n--- Server Setup Complete ---\")\n",
        "if public_url:\n",
        "    print(f\"Access your game POC at: {public_url}\")\n",
        "else:\n",
        "    print(\"Failed to start ngrok tunnel. Server might be running but not accessible publicly.\")\n",
        "print(\"Flask server is running in the background.\")\n",
        "print(\"LLM endpoint is available at /api/llm_move (relative to the ngrok URL)\")\n",
        "print(\"Make sure you have uploaded your index.html, game.js, and phaser.min.js into the 'game_files' directory in Colab.\")\n",
        "# Keep the Colab cell running...\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lTHo_bnq-pns"
      },
      "outputs": [],
      "source": [
        "!curl http://127.0.0.1:8080/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FIM1RbjEA5jy"
      },
      "outputs": [],
      "source": [
        "!lsof -i :8080"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I_dBlttoA82Z"
      },
      "outputs": [],
      "source": [
        "!kill -9 6"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}